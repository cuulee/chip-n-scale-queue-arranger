stage: production
stackName: inference-backend
region: us-east-1
bucket: my-bucket # existing s3 bucket to store deployment artifacts

tags:
  project: name-of-project
  
rds:
  name: results
  username: postgres
  password: mysecretpassword
  storage: 20
  instanceType: 'db.t2.medium'
  port: '5432'

ecs:
  availabilityZone: us-east-1a
  maxInstances: 1
  desiredInstances: 1
  keyPairName: my-key-pair
  instanceType: t2.nano # replace with a GPU instance for faster predictions (and higher costs)
  image: tensorflow/serving:latest # docker image containing your inference model built with TF Serving
  port: 8501 # Port to redirect input request (e.g 8501 for TF Serving)
  memory: 1000 # replace with the memory required by your TF Serving docker image

predictionPath: '/v1/models/your_model:predict' # path to your model on the TF Serving docker image; don't include :predict
